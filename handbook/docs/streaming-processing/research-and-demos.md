# Research v√† Demos v·ªÅ Streaming Processing

## T·ªïng quan

C√°c demo v·ªÅ Streaming Processing bao g·ªìm x·ª≠ l√Ω lu·ªìng d·ªØ li·ªáu th·ªùi gian th·ª±c, windowing, aggregation, stateful processing, v√† c√°c pattern x·ª≠ l√Ω lu·ªìng kh√°c nhau.

## C√¥ng ngh·ªá s·ª≠ d·ª•ng

- **Apache Kafka Streams**: Stream processing library
- **Apache Flink**: Distributed stream processing framework
- **Redis Streams**: Lightweight stream processing
- **Node.js Streams**: Native streaming API
- **RxJS**: Reactive programming library
- **KafkaJS**: Kafka client v·ªõi stream support
- **Transform Streams**: Data transformation pipelines

## C√°c Demo

### Demo 1: Basic Stream Processing - Real-time Counting

**C√¥ng ngh·ªá:** Kafka Consumer, in-memory state

**C√°ch tri·ªÉn khai:**
- Consumer l·∫Øng nghe events li√™n t·ª•c t·ª´ Kafka topic
- M·ªói event ƒë·∫øn, tƒÉng counter (stateful processing)
- Kh√¥ng ph·∫£i ch·ªâ nh·∫≠n message r·ªìi x·ª≠ l√Ω xong (messaging), m√† duy tr√¨ state v√† t√≠nh to√°n li√™n t·ª•c
- Gi·ªëng nh∆∞ ng∆∞·ªùi b√°n h√†ng ƒë·∫øm s·ªë kh√°ch trong ƒë·∫ßu: kh√°ch 1, kh√°ch 2, kh√°ch 3...
- State ƒë∆∞·ª£c l∆∞u trong memory, c·∫≠p nh·∫≠t real-time
- Gi·ªëng nh∆∞ ƒë·∫øm s·ªë ng∆∞·ªùi qua c·ªïng, c·ª© c√≥ ng∆∞·ªùi ƒëi qua l√† c·ªông th√™m 1

**C√°ch test:**
```bash
# Start analytics service (stream processor)
cd backend/apps/analytics-service
pnpm run start:dev

# Consumer logs: "Total orders: 0"

# G·ª≠i events
curl -X POST http://localhost:3000/api/orders -d '{"amount": 100}'
# Logs: "Total orders: 1"

curl -X POST http://localhost:3000/api/orders -d '{"amount": 200}'
# Logs: "Total orders: 2"

curl -X POST http://localhost:3000/api/orders -d '{"amount": 150}'
# Logs: "Total orders: 3"

# Xem real-time analytics
curl http://localhost:3001/analytics
# Response: { totalOrders: 3, totalRevenue: 450 }
```

### Demo 2: Windowing - Time-based Aggregation

**C√¥ng ngh·ªá:** Tumbling window, sliding window

**C√°ch tri·ªÉn khai:**
- Chia time th√†nh c√°c windows (c·ª≠a s·ªï th·ªùi gian): v√≠ d·ª• m·ªói window 1 ph√∫t
- Tumbling window: Kh√¥ng overlap, window 1 (0-60s), window 2 (60-120s)
- Sliding window: C√≥ overlap, window c·ª© 10s l·∫°i t·∫°o m·ªõi, m·ªói window k√©o d√†i 60s
- T√≠nh metrics cho m·ªói window (s·ªë orders/ph√∫t, revenue/ph√∫t)
- Sau khi window ƒë√≥ng, emit k·∫øt qu·∫£
- D√πng ƒë·ªÉ ph√°t hi·ªán patterns: peak hours, traffic spikes
- Gi·ªëng nh∆∞ ƒë·∫øm s·ªë xe qua ng√£ t∆∞ m·ªói ph√∫t ƒë·ªÉ bi·∫øt gi·ªù cao ƒëi·ªÉm

**C√°ch test:**
```bash
# T·∫°o stream processor v·ªõi tumbling window (60 gi√¢y)
class TumblingWindowProcessor {
  private currentWindow = {
    start: Date.now(),
    count: 0,
    revenue: 0
  };

  handleEvent(event) {
    const now = Date.now();
    
    // Check if window expired
    if (now - this.currentWindow.start > 60000) {
      this.emitWindow(this.currentWindow);
      this.currentWindow = { start: now, count: 0, revenue: 0 };
    }
    
    // Add to current window
    this.currentWindow.count++;
    this.currentWindow.revenue += event.amount;
  }
}

# G·ª≠i nhi·ªÅu events trong 2 ph√∫t
for i in {1..30}; do
  curl -X POST http://localhost:3000/api/orders -d '{"amount": 100}'
  sleep 2
done

# Output sau 60s: "Window [0-60s]: 30 orders, $3000"
# Output sau 120s: "Window [60-120s]: 30 orders, $3000"
```

### Demo 3: Stateful Aggregation - Running Totals

**C√¥ng ngh·ªá:** In-memory state, Redis for persistence

**C√°ch tri·ªÉn khai:**
- Duy tr√¨ state li√™n t·ª•c: t·ªïng revenue, trung b√¨nh order value, min/max
- M·ªói event m·ªõi, c·∫≠p nh·∫≠t t·∫•t c·∫£ metrics
- State kh√¥ng reset sau m·ªói event (kh√°c v·ªõi stateless)
- C·∫ßn l∆∞u state ƒë·ªÉ recover khi restart (d√πng Redis ho·∫∑c Kafka state stores)
- T√≠nh to√°n incremental: kh√¥ng c·∫ßn query l·∫°i to√†n b·ªô data
- Gi·ªëng nh∆∞ s·ªë d∆∞ t√†i kho·∫£n: c·ª© c√≥ giao d·ªãch l√† c·∫≠p nh·∫≠t ngay

**C√°ch test:**
```bash
# Analytics service duy tr√¨ running state
class StatefulAggregator {
  private state = {
    totalRevenue: 0,
    orderCount: 0,
    minOrder: Infinity,
    maxOrder: 0
  };

  handlePayment(event) {
    this.state.totalRevenue += event.amount;
    this.state.orderCount++;
    this.state.minOrder = Math.min(this.state.minOrder, event.amount);
    this.state.maxOrder = Math.max(this.state.maxOrder, event.amount);
    
    // Log running state
    console.log('Running Total:', this.state.totalRevenue);
    console.log('Avg Order:', this.state.totalRevenue / this.state.orderCount);
  }
}

# G·ª≠i orders v·ªõi amounts kh√°c nhau
curl -X POST http://localhost:3000/api/orders -d '{"amount": 100}'
# Running Total: 100, Avg: 100

curl -X POST http://localhost:3000/api/orders -d '{"amount": 500}'
# Running Total: 600, Avg: 300

curl -X POST http://localhost:3000/api/orders -d '{"amount": 200}'
# Running Total: 800, Avg: 267
```

### Demo 4: Stream Joining - Combine Multiple Streams

**C√¥ng ngh·ªá:** Kafka Streams, join operations

**C√°ch tri·ªÉn khai:**
- C√≥ 2 streams ri√™ng bi·ªát: orders stream v√† payments stream
- Join 2 streams d·ª±a tr√™n key chung (orderId)
- T·∫°o enriched stream ch·ª©a th√¥ng tin t·ª´ c·∫£ 2
- C√≥ th·ªÉ inner join (c·∫£ 2 ph·∫£i c√≥), left join (order c√≥, payment c√≥ th·ªÉ kh√¥ng)
- Time window cho join: join events trong c√πng kho·∫£ng th·ªùi gian (v√≠ d·ª• 5 ph√∫t)
- Gi·ªëng nh∆∞ gh√©p 2 m·∫£nh gi·∫•y: ƒë∆°n h√†ng + h√≥a ƒë∆°n ƒë·ªÉ c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß

**C√°ch test:**
```bash
# Stream join processor
class StreamJoiner {
  private orders = new Map();
  private payments = new Map();

  handleOrder(order) {
    this.orders.set(order.id, order);
    this.tryJoin(order.id);
  }

  handlePayment(payment) {
    this.payments.set(payment.orderId, payment);
    this.tryJoin(payment.orderId);
  }

  tryJoin(orderId) {
    const order = this.orders.get(orderId);
    const payment = this.payments.get(orderId);
    
    if (order && payment) {
      const enriched = { ...order, ...payment };
      this.emit('order-payment-complete', enriched);
      
      // Cleanup
      this.orders.delete(orderId);
      this.payments.delete(orderId);
    }
  }
}

# G·ª≠i order
curl -X POST http://localhost:3000/api/orders \
  -d '{"id": "order-1", "amount": 100}'
# Ch∆∞a c√≥ output (ch·ªù payment)

# G·ª≠i payment
curl -X POST http://localhost:3000/api/payments \
  -d '{"orderId": "order-1", "paymentId": "pay-1"}'
# Output: "Enriched: {id: order-1, amount: 100, paymentId: pay-1}"
```

### Demo 5: Event Time vs Processing Time

**C√¥ng ngh·ªá:** Kafka timestamp, watermarks

**C√°ch tri·ªÉn khai:**
- Event time: Th·ªùi gian event th·ª±c s·ª± x·∫£y ra (trong event payload)
- Processing time: Th·ªùi gian h·ªá th·ªëng x·ª≠ l√Ω event (c√≥ th·ªÉ ch·∫≠m do network, lag)
- Events c√≥ th·ªÉ ƒë·∫øn kh√¥ng ƒë√∫ng th·ª© t·ª± (out-of-order)
- D√πng watermarks ƒë·ªÉ bi·∫øt events c≈© ƒë√£ ƒë·∫øn h·∫øt ch∆∞a
- X·ª≠ l√Ω theo event time ch√≠nh x√°c h∆°n (d√πng cho billing, analytics)
- Gi·ªëng nh∆∞ s·∫Øp x·∫øp th∆∞ theo ng√†y g·ª≠i (event time), kh√¥ng ph·∫£i ng√†y nh·∫≠n (processing time)

**C√°ch test:**
```bash
# Processor v·ªõi event time
class EventTimeProcessor {
  handleEvent(event) {
    const eventTime = new Date(event.timestamp);
    const processingTime = new Date();
    const lag = processingTime - eventTime;
    
    console.log(`Event time: ${eventTime}`);
    console.log(`Processing time: ${processingTime}`);
    console.log(`Lag: ${lag}ms`);
    
    // Use event time for windowing
    this.addToWindow(event, eventTime);
  }
}

# G·ª≠i events v·ªõi timestamps kh√°c nhau
curl -X POST http://localhost:3000/api/orders \
  -d '{"id": "order-1", "timestamp": "2026-01-16T10:00:00Z", "amount": 100}'
# Event time: 10:00:00, Processing time: 10:00:05, Lag: 5000ms

# G·ª≠i event c≈© (out of order)
curl -X POST http://localhost:3000/api/orders \
  -d '{"id": "order-2", "timestamp": "2026-01-16T09:59:00Z", "amount": 200}'
# Event time: 09:59:00 (c≈© h∆°n order-1), Processing time: 10:00:10
# Processor v·∫´n x·ª≠ l√Ω ƒë√∫ng theo event time
```

### Demo 6: Backpressure Handling

**C√¥ng ngh·ªá:** Node.js Streams, buffering

**C√°ch tri·ªÉn khai:**
- Producer t·∫°o events nhanh h∆°n consumer x·ª≠ l√Ω (consumer b·ªã qu√° t·∫£i)
- Backpressure: Consumer b√°o hi·ªáu "ch·∫≠m l·∫°i, t√¥i ch∆∞a k·ªãp x·ª≠ l√Ω"
- Buffer ƒë·ªÉ l∆∞u events t·∫°m th·ªùi khi consumer ch·∫≠m
- N·∫øu buffer ƒë·∫ßy, c√≥ th·ªÉ drop events (at-most-once) ho·∫∑c ch·∫∑n producer
- Node.js Streams t·ª± ƒë·ªông handle backpressure v·ªõi pause/resume
- Gi·ªëng nh∆∞ ƒë∆∞·ªùng ·ªëng n∆∞·ªõc: n·∫øu d√≤ng ch·∫£y m·∫°nh qu√°, c·∫ßn gi·∫£m √°p l·ª±c

**C√°ch test:**
```bash
# Consumer ch·∫≠m v·ªõi backpressure
const { Transform } = require('stream');

const slowProcessor = new Transform({
  objectMode: true,
  highWaterMark: 5,  // Buffer t·ªëi ƒëa 5 items
  
  async transform(event, encoding, callback) {
    // Gi·∫£ l·∫≠p x·ª≠ l√Ω ch·∫≠m
    await new Promise(resolve => setTimeout(resolve, 1000));
    
    console.log('Processed:', event);
    callback(null, event);
  }
});

// Producer g·ª≠i nhanh
for (let i = 0; i < 100; i++) {
  const canContinue = slowProcessor.write({ id: i });
  
  if (!canContinue) {
    console.log('Backpressure! Pausing producer...');
    await new Promise(resolve => slowProcessor.once('drain', resolve));
  }
}

# Output:
# Processed: {id: 0}
# Processed: {id: 1}
# ...
# Backpressure! Pausing producer... (khi buffer ƒë·∫ßy)
# Processed: {id: 5}
# Backpressure! Pausing producer...
```

### Demo 7: Real-time Filtering v√† Transformation

**C√¥ng ngh·ªá:** RxJS operators, Kafka Streams

**C√°ch tri·ªÉn khai:**
- Nh·∫≠n stream events, l·ªçc ra nh·ªØng events quan t√¢m (filter)
- Transform data: thay ƒë·ªïi format, enrich v·ªõi th√¥ng tin th√™m, t√≠nh to√°n
- Chain nhi·ªÅu operations: filter ‚Üí map ‚Üí reduce ‚Üí output
- Declarative style v·ªõi RxJS operators
- V√≠ d·ª•: Ch·ªâ quan t√¢m high-value orders (> $1000), chuy·ªÉn ƒë·ªïi currency, t√≠nh tax
- Gi·ªëng nh∆∞ d√¢y chuy·ªÅn s·∫£n xu·∫•t: l·ªçc, ch·∫ø bi·∫øn, ƒë√≥ng g√≥i

**C√°ch test:**
```bash
# RxJS stream pipeline
import { from } from 'rxjs';
import { filter, map, reduce } from 'rxjs/operators';

const orderStream$ = from(kafkaConsumer);

orderStream$
  .pipe(
    // Filter: ch·ªâ l·∫•y high-value orders
    filter(order => order.amount > 1000),
    
    // Transform: convert USD to VND
    map(order => ({
      ...order,
      amountVND: order.amount * 24000
    })),
    
    // Aggregate: t√≠nh t·ªïng high-value orders
    reduce((total, order) => total + order.amountVND, 0)
  )
  .subscribe(total => {
    console.log('Total high-value orders (VND):', total);
  });

# G·ª≠i mixed orders
curl -X POST http://localhost:3000/api/orders -d '{"amount": 500}'   # Filtered out
curl -X POST http://localhost:3000/api/orders -d '{"amount": 1500}'  # Accepted
curl -X POST http://localhost:3000/api/orders -d '{"amount": 800}'   # Filtered out
curl -X POST http://localhost:3000/api/orders -d '{"amount": 2000}'  # Accepted

# Output: "Total high-value orders (VND): 84,000,000"
# (1500 + 2000) * 24000
```

### Demo 8: Stream Deduplication

**C√¥ng ngh·ªá:** Redis Set, Kafka exactly-once

**C√°ch tri·ªÉn khai:**
- Trong h·ªá th·ªëng ph√¢n t√°n, events c√≥ th·ªÉ b·ªã duplicate (network retry, producer retry)
- C·∫ßn ph√°t hi·ªán v√† lo·∫°i b·ªè duplicates ƒë·ªÉ kh√¥ng x·ª≠ l√Ω 2 l·∫ßn
- D√πng unique event ID, l∆∞u trong Set (Redis ho·∫∑c in-memory)
- Check m·ªói event: ƒë√£ th·∫•y ID n√†y ch∆∞a? N·∫øu r·ªìi th√¨ skip
- Set c√≥ TTL ƒë·ªÉ kh√¥ng t·ªën memory m√£i (gi·ªØ 1 gi·ªù ho·∫∑c 1 ng√†y)
- Gi·ªëng nh∆∞ ƒë√≥ng d·∫•u "ƒê√£ x·ª≠ l√Ω" tr√™n gi·∫•y t·ªù ƒë·ªÉ kh√¥ng l√†m l·∫°i

**C√°ch test:**
```bash
# Deduplication processor
class DeduplicationProcessor {
  private seenIds = new Set();

  async handleEvent(event) {
    // Check if already seen
    if (this.seenIds.has(event.id)) {
      console.log('Duplicate detected:', event.id);
      return; // Skip
    }
    
    // Mark as seen
    this.seenIds.add(event.id);
    
    // Process
    await this.process(event);
  }
}

# G·ª≠i event
curl -X POST http://localhost:3000/api/orders \
  -d '{"id": "order-123", "amount": 100}'
# Output: "Processing order-123"

# G·ª≠i duplicate
curl -X POST http://localhost:3000/api/orders \
  -d '{"id": "order-123", "amount": 100}'
# Output: "Duplicate detected: order-123" (not processed)

# G·ª≠i new event
curl -X POST http://localhost:3000/api/orders \
  -d '{"id": "order-456", "amount": 200}'
# Output: "Processing order-456"
```

### Demo 9: Complex Event Processing (CEP)

**C√¥ng ngh·ªá:** Pattern matching, temporal queries

**C√°ch tri·ªÉn khai:**
- Ph√°t hi·ªán patterns ph·ª©c t·∫°p trong stream events
- V√≠ d·ª•: Ph√°t hi·ªán fraud - 3 l·∫ßn payment failed trong 5 ph√∫t t·ª´ c√πng 1 user
- Duy tr√¨ state c·ªßa events g·∫ßn ƒë√¢y trong window
- Khi pattern match, trigger action (alert, block user)
- Temporal queries: "A followed by B within 10 seconds"
- Gi·ªëng nh∆∞ trinh s√°t ph√°t hi·ªán h√†nh vi ƒë√°ng ng·ªù t·ª´ chu·ªói h√†nh ƒë·ªông

**C√°ch test:**
```bash
# Fraud detection v·ªõi CEP
class FraudDetector {
  private failedAttempts = new Map(); // userId -> [timestamps]

  async handlePaymentFailed(event) {
    const userId = event.userId;
    const now = Date.now();
    
    // Get recent failures
    const attempts = this.failedAttempts.get(userId) || [];
    
    // Remove old attempts (> 5 minutes)
    const recentAttempts = attempts.filter(t => now - t < 300000);
    
    // Add current attempt
    recentAttempts.push(now);
    this.failedAttempts.set(userId, recentAttempts);
    
    // Check pattern: 3 failures in 5 minutes
    if (recentAttempts.length >= 3) {
      await this.triggerFraudAlert(userId);
      console.log('üö® FRAUD ALERT: User', userId, 'has 3 failed payments in 5 min');
    }
  }
}

# Simulate fraud pattern
curl -X POST http://localhost:3000/api/payments \
  -d '{"userId": "user-1", "status": "failed"}'
# Attempt 1

sleep 30
curl -X POST http://localhost:3000/api/payments \
  -d '{"userId": "user-1", "status": "failed"}'
# Attempt 2

sleep 30
curl -X POST http://localhost:3000/api/payments \
  -d '{"userId": "user-1", "status": "failed"}'
# Attempt 3 ‚Üí üö® FRAUD ALERT
```

### Demo 10: Lambda Architecture - Batch + Stream

**C√¥ng ngh·ªá:** Batch processing (Spark) + Stream processing (Kafka)

**C√°ch tri·ªÉn khai:**
- Batch layer: X·ª≠ l√Ω to√†n b·ªô historical data m·ªói ng√†y (ch√≠nh x√°c nh∆∞ng ch·∫≠m)
- Speed layer: X·ª≠ l√Ω real-time data (nhanh nh∆∞ng c√≥ th·ªÉ thi·∫øu)
- Serving layer: Merge k·∫øt qu·∫£ t·ª´ c·∫£ 2 layers
- Query = Batch results (ƒë·∫øn h√¥m qua) + Stream results (t·ª´ h√¥m qua ƒë·∫øn gi·ªù)
- ƒê·∫£m b·∫£o v·ª´a c√≥ accuracy (batch) v·ª´a c√≥ low latency (stream)
- Gi·ªëng nh∆∞ b√°o c√°o t√†i ch√≠nh: c√≥ b√°o c√°o th√°ng (batch) v√† s·ªë d∆∞ hi·ªán t·∫°i (stream)

**C√°ch test:**
```bash
# Batch layer (ch·∫°y daily)
@Cron('0 0 * * *')  // Midnight
async batchProcess() {
  const yesterday = new Date(Date.now() - 86400000);
  
  const orders = await this.db.query(`
    SELECT SUM(amount) as total
    FROM orders
    WHERE date < ?
  `, [yesterday]);
  
  await this.cache.set('batch_total', orders.total);
  console.log('Batch processed:', orders.total);
}

# Speed layer (real-time)
@EventPattern('order-created')
async streamProcess(order) {
  const realtimeTotal = await this.cache.get('realtime_total') || 0;
  await this.cache.set('realtime_total', realtimeTotal + order.amount);
}

# Serving layer (merge)
@Get('total-revenue')
async getTotalRevenue() {
  const batchTotal = await this.cache.get('batch_total') || 0;
  const realtimeTotal = await this.cache.get('realtime_total') || 0;
  
  return {
    total: batchTotal + realtimeTotal,
    batch: batchTotal,
    realtime: realtimeTotal
  };
}

# Test
curl http://localhost:3000/api/total-revenue
# Response:
# {
#   "total": 125000,
#   "batch": 100000,    // From yesterday
#   "realtime": 25000   // Today so far
# }
```

## C√°ch ch·∫°y c√°c demo

### 1. Start Kafka

```bash
cd backend
docker-compose up -d kafka zookeeper
```

### 2. Start Analytics Service (Stream Processor)

```bash
cd backend/apps/analytics-service
pnpm run start:dev

# Service s·∫Ω b·∫Øt ƒë·∫ßu consume events v√† x·ª≠ l√Ω streaming
```

### 3. G·ª≠i events ƒë·ªÉ xem stream processing

```bash
# G·ª≠i orders
for i in {1..10}; do
  curl -X POST http://localhost:3000/api/orders \
    -H "Content-Type: application/json" \
    -d "{\"userId\": \"user-$i\", \"amount\": $((RANDOM % 1000 + 100))}"
  sleep 1
done

# Xem analytics service logs ƒë·ªÉ th·∫•y real-time processing
docker logs -f analytics-service
```

### 4. Query streaming analytics

```bash
# Get current analytics
curl http://localhost:3001/analytics

# Response:
# {
#   "totalOrders": 10,
#   "totalRevenue": 5487,
#   "avgOrderValue": 548.7,
#   "successRate": 90.0,
#   "ordersPerMinute": [10]
# }
```

### 5. Test windowing

```bash
# Script g·ª≠i events ƒë·ªÅu ƒë·∫∑n m·ªói 5 gi√¢y trong 3 ph√∫t
for i in {1..36}; do
  curl -X POST http://localhost:3000/api/orders \
    -d '{"amount": 100}'
  sleep 5
done

# Xem window aggregations m·ªói ph√∫t
# Minute 1: 12 orders
# Minute 2: 12 orders
# Minute 3: 12 orders
```

### 6. Test stream joins

```bash
# Terminal 1: Monitor joined stream
node stream-join-monitor.js

# Terminal 2: Send order
curl -X POST http://localhost:3000/api/orders \
  -d '{"id": "order-1", "amount": 100}'

# Terminal 3: Send payment
curl -X POST http://localhost:3000/api/payments \
  -d '{"orderId": "order-1", "paymentId": "pay-1"}'

# Terminal 1 shows: "Joined: {orderId: order-1, amount: 100, paymentId: pay-1}"
```

### 7. Load testing stream processing

```bash
# G·ª≠i high volume events
npm install -g artillery

artillery quick \
  --count 1000 \
  --num 100 \
  -p http://localhost:3000/api/orders

# Monitor analytics service
docker stats analytics-service
watch -n 1 'curl -s http://localhost:3001/analytics'
```

### 8. Test backpressure

```bash
# Script v·ªõi slow consumer
node slow-consumer.js &

# Fast producer
for i in {1..1000}; do
  curl -X POST http://localhost:3000/api/orders -d "{\"id\": $i}"
done

# Xem logs: "Backpressure detected, pausing..."
```

### 9. Monitor Kafka consumer lag

```bash
# Check stream processing lag
docker exec kafka kafka-consumer-groups.sh \
  --describe \
  --group analytics-service-group \
  --bootstrap-server localhost:9092

# Output:
# TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
# order-created   0          1000            1050            50
# payment-completed 0        950             1000            50

# Lag > 0 nghƒ©a l√† stream processor ƒëang ch·∫≠m h∆°n producer
```

### 10. Visualize stream metrics (optional)

```bash
# Start Grafana + Prometheus
docker-compose up -d grafana prometheus

# Import dashboard
open http://localhost:3000/grafana

# Dashboards show:
# - Events per second
# - Processing latency
# - Throughput
# - Consumer lag
# - Error rates
```

## T√†i li·ªáu tham kh·∫£o

- [Kafka Streams Documentation](https://kafka.apache.org/documentation/streams/)
- [Apache Flink](https://flink.apache.org/)
- [Stream Processing 101](https://www.oreilly.com/radar/the-world-beyond-batch-streaming-101/)
- [RxJS Operators](https://rxjs.dev/guide/operators)
- [Node.js Streams](https://nodejs.org/api/stream.html)
- [Lambda Architecture](http://lambda-architecture.net/)
- [Designing Data-Intensive Applications](https://dataintensive.net/)
